{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff4bce",
   "metadata": {},
   "source": [
    "# Tensorflow2.0 小练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e84fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as npm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aefce1",
   "metadata": {},
   "source": [
    "## 实现softmax函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11818356",
   "metadata": {},
   "source": [
    "![title](img/Softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3739d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np\n",
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    prob_x = x_exp / x_sum\n",
    "    return prob_x\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "(softmax(test_data) - tf.nn.softmax(test_data, axis=-1).numpy())**2 <0.0001  #去掉了.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d661580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmaxF(x):\n",
    "    x_exp = tf.math.exp(x)\n",
    "    x_sum = tf.reduce_sum(x_exp, axis=1, keepdims=True)\n",
    "    prob_x = x_exp / x_sum\n",
    "    return prob_x\n",
    "(softmaxF(test_data).numpy() - tf.nn.softmax(test_data, axis=-1).numpy())**2 <0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2f795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 实现sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9edba",
   "metadata": {},
   "source": [
    "![title](img/Sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e717b945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np\n",
    "def sigmoid(x):\n",
    "    x =1 + (1 / np.exp(x))\n",
    "    prob_x = 1 / x\n",
    "    return prob_x\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "(sigmoid(test_data) - tf.nn.sigmoid(test_data).numpy())**2 < 0.0001  #去掉了.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c36a590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf\n",
    "def sigmoidF(x):\n",
    "    prob_x =1/(1+ (1/tf.math.exp(x)))\n",
    "    return prob_x\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "# print(tf.nn.softmax(test_data, axis=-1).numpy(),'T')\n",
    "(sigmoidF(test_data).numpy() - tf.nn.sigmoid(test_data).numpy())**2 < 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fd6d8",
   "metadata": {},
   "source": [
    "## 实现 softmax 交叉熵loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72c341",
   "metadata": {},
   "source": [
    "![title](img/softmaxH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5846dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np\n",
    "def softmax_ce(x, label):\n",
    "    loss = -np.sum(np.nan_to_num(label*np.log(x)),axis=1) \n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "prob = tf.nn.softmax(test_data)\n",
    "label = np.zeros_like(test_data)\n",
    "label[np.arange(10), np.random.randint(0, 5, size=10)]=1.\n",
    "((tf.nn.softmax_cross_entropy_with_logits(label, test_data) #tf.reduce_mean()会把张量格式改了，很奇怪!\n",
    "  - softmax_ce(prob, label))**2 < 0.0001).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727df8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf\n",
    "def softmax_ceF(x, label):\n",
    "    ##########\n",
    "    '''实现 softmax 交叉熵loss函数， 不允许用tf自带的softmax_cross_entropy函数'''\n",
    "    ##########\n",
    "    losses = -tf.reduce_sum(label*tf.math.log(x),axis=1)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10, 5])\n",
    "prob = tf.nn.softmax(test_data)\n",
    "label = np.zeros_like(test_data)\n",
    "label[np.arange(10), np.random.randint(0, 5, size=10)]=1.\n",
    "\n",
    "((tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(label, test_data))\n",
    "  - softmax_ceF(prob, label))**2 < 0.0001).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7504495",
   "metadata": {},
   "source": [
    "## 实现 sigmoid 交叉熵loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced5a90",
   "metadata": {},
   "source": [
    "![title](img/sigmoidH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "023a91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np\n",
    "def sigmoid_ce(x, label):\n",
    "    loss = -np.sum(np.nan_to_num(label*np.log(x)+(1-label)*np.log(1-x)))\n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10])\n",
    "prob = tf.nn.sigmoid(test_data)\n",
    "label = np.random.randint(0, 2, 10).astype(test_data.dtype)\n",
    "a = tf.nn.sigmoid_cross_entropy_with_logits(label, test_data).numpy()\n",
    "((np.sum(a)- sigmoid_ce(prob, label))**2 < 0.0001)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae9ee295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf\n",
    "def sigmoid_ceF(x, label):\n",
    "    losses = -tf.reduce_sum(label*tf.math.log(x)+(1.-label)*tf.math.log(1-x))/len(x)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    return loss\n",
    "\n",
    "test_data = np.random.normal(size=[10])\n",
    "prob = tf.nn.sigmoid(test_data)\n",
    "label = np.random.randint(0, 2, 10).astype(test_data.dtype)\n",
    "\n",
    "((tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(label, test_data))- sigmoid_ceF(prob, label))**2 < 0.0001).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxx",
   "language": "python",
   "name": "sdxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
